{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model attn-only-4l into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from functools import partial\n",
    "import random\n",
    "\n",
    "model = HookedTransformer.from_pretrained('attn-only-4l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def get_prompts_list(dataset_name: str, n_prompts: int, shuffle_buffer_size: int, shuffle_seed: int):\n",
    "    print(f\"Loading {n_prompts} prompts from {dataset_name}...\")\n",
    "    file_name = f\"{dataset_name}-{n_prompts}-seed{shuffle_seed}-buffer{shuffle_buffer_size}.pkl\"\n",
    "    file_path = \"./data\" / Path(file_name) # Change based on user\n",
    "    if file_path.exists():\n",
    "        print(\"Using pickled prompts...\")\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    print(\"Downloading from HuggingFace...\")\n",
    "    prompts_list = []\n",
    "    ds_unsfuffled = load_dataset(f\"NeelNanda/{dataset_name}\", streaming=True, split=\"train\")\n",
    "    ds = ds_unsfuffled.shuffle(buffer_size=shuffle_buffer_size, seed=shuffle_seed)\n",
    "    ds_iter = iter(ds)\n",
    "    for _ in trange(n_prompts):\n",
    "        prompts_list.append(next(ds_iter)[\"tokens\"])\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(prompts_list, f)\n",
    "    return prompts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 80 prompts from c4-tokenized-2b...\n",
      "Using pickled prompts...\n",
      "Loading 20 prompts from code-tokenized...\n",
      "Using pickled prompts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(65)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_TOTAL_PROMPTS = 100\n",
    "N_C4_TOTAL_PROMPTS = int(0.8 * N_TOTAL_PROMPTS)\n",
    "N_CODE_TOTAL_PROMPTS = N_TOTAL_PROMPTS - N_C4_TOTAL_PROMPTS\n",
    "DS_SHUFFLE_SEED, DS_SHUFFLE_BUFFER_SIZE = 5235, N_TOTAL_PROMPTS // 10\n",
    "\n",
    "def shuffle_tensor(tensor, dim):\n",
    "    torch.manual_seed(DS_SHUFFLE_SEED)\n",
    "    torch.cuda.manual_seed(DS_SHUFFLE_SEED)\n",
    "    return tensor[torch.randperm(tensor.shape[dim])]\n",
    "\n",
    "def get_prompts_t():\n",
    "    shuffle_kwargs = dict(shuffle_buffer_size=DS_SHUFFLE_BUFFER_SIZE, shuffle_seed=DS_SHUFFLE_SEED)\n",
    "    c4_prompts_list = get_prompts_list(\"c4-tokenized-2b\", n_prompts=N_C4_TOTAL_PROMPTS, **shuffle_kwargs)\n",
    "    code_prompts_list = get_prompts_list(\"code-tokenized\", n_prompts=N_CODE_TOTAL_PROMPTS, **shuffle_kwargs)\n",
    "    prompts_t = torch.tensor(\n",
    "        c4_prompts_list + code_prompts_list\n",
    "    )\n",
    "    return shuffle_tensor(prompts_t, dim=0)\n",
    "\n",
    "def get_token_counts(prompts_t_):\n",
    "    unique_tokens, tokens_counts_ = torch.unique(prompts_t_, return_counts=True)\n",
    "    tokens_counts = torch.zeros(model.cfg.d_vocab, dtype=torch.int64, device=device)\n",
    "    tokens_counts[unique_tokens] = tokens_counts_.to(device)\n",
    "    return tokens_counts\n",
    "\n",
    "prompts_t = get_prompts_t()\n",
    "token_counts = get_token_counts(prompts_t)\n",
    "\n",
    "MIN_TOKEN_COUNT = N_TOTAL_PROMPTS // 1_000\n",
    "tokens = torch.arange(model.cfg.d_vocab, device=device, dtype=torch.int32)\n",
    "tokens = tokens[token_counts >= MIN_TOKEN_COUNT]\n",
    "tokens_set = set(tokens.tolist())\n",
    "prompts_t[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_patterns(model, prompts, layer: int, head: int):\n",
    "    patterns = None\n",
    "    def hook_get_pattern(act, hook):\n",
    "        # batch, dst, src\n",
    "        nonlocal patterns\n",
    "        patterns = act[:, head]\n",
    "\n",
    "    model.reset_hooks()\n",
    "    model.add_hook(f\"blocks.{layer}.attn.hook_pattern\", hook_get_pattern)\n",
    "    model(prompts, stop_at_layer=layer+1)\n",
    "    model.reset_hooks()\n",
    "\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def get_patterns(model, \n",
    "                prompts, # n_examples, n_ctx\n",
    "                layer: int, \n",
    "                head: int, \n",
    "                mb_size: int):\n",
    "\n",
    "    # Store attention patterns as a list of dicts\n",
    "    patterns = []\n",
    "    \n",
    "    # Enumerate over each batch of prompts\n",
    "    for prompts_mb_idx in trange(0, prompts.shape[0], mb_size):\n",
    "        prompts_mb = prompts[prompts_mb_idx:prompts_mb_idx+mb_size]\n",
    "\n",
    "        # Get attn patterns of a specific head, ignoring first ignore_first_n_pos dst pos or rows\n",
    "        raw_patterns_mb = get_raw_patterns(model, prompts_mb, layer, head)\n",
    "        # TODO: ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
