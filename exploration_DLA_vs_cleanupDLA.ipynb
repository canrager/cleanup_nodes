{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Import modules\n",
    "import torch\n",
    "import einops\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "from transformer_lens.utils import get_act_name\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from typing import List, Callable, Optional\n",
    "from plotting import (\n",
    "    get_fig_head_to_mlp_neuron,\n",
    "    get_fig_head_to_mlp_neuron_by_layer,\n",
    "    get_fig_head_to_selected_mlp_neuron,\n",
    "    ntensor_to_long,\n",
    ")\n",
    "from load_data import get_prompts_t\n",
    "from utils import (\n",
    "    projection,\n",
    "    cos_similarity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings and variables\n",
    "sns.set()\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cpu\"\n",
    "\n",
    "IPSUM = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\"\n",
    "# IPSUM = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-4l into HookedTransformer\n",
      "Moving model to device:  cpu\n",
      "Loading 80 prompts from c4-tokenized-2b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:02<00:00, 27.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 prompts from code-tokenized...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  6.69it/s]\n"
     ]
    }
   ],
   "source": [
    "#%% Setup model & load data\n",
    "model = HookedTransformer.from_pretrained('gelu-4l')\n",
    "model.cfg.use_attn_result = True\n",
    "model.to(device)\n",
    "\n",
    "prompts_t = get_prompts_t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"text\": 4 * IPSUM + \" It's in the cupboard, either on the top or the\",\n",
    "        \"correct\": \" bottom\",\n",
    "        \"incorrect\": \" top\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": 5 * IPSUM + \" I went to university at Michigan\",\n",
    "        \"correct\": \" State\",\n",
    "        \"incorrect\": \" University\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": IPSUM + \" class MyClass:\\n\\tdef\",\n",
    "        \"correct\": \" __\",\n",
    "        \"incorrect\": \" on\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": 6 * IPSUM + \"The church I go to is the Seventh-day Adventist\",\n",
    "        \"correct\": \" Church\",\n",
    "        \"incorrect\": \" Advent\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = model.run_with_cache(examples[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 6.3408, -6.8591, -6.8589,  ..., -6.8638, -6.8522, -6.8653],\n",
       "          [ 3.7090, -5.5066, -5.5378,  ..., -5.5730, -5.5053, -5.5130],\n",
       "          [ 3.1071, -6.9010, -6.8954,  ..., -6.9342, -6.9581, -6.9972],\n",
       "          ...,\n",
       "          [ 4.4170, -6.8391, -6.9054,  ..., -6.8645, -6.8689, -6.8825],\n",
       "          [ 1.5867, -6.7227, -6.7652,  ..., -6.7718, -6.8076, -6.7640],\n",
       "          [ 2.6618, -6.1671, -6.2009,  ..., -6.1731, -6.2137, -6.2051]]]),\n",
       " ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.attn.hook_result', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.hook_mlp_in', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.attn.hook_result', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.hook_mlp_in', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.attn.hook_result', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.hook_mlp_in', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.attn.hook_result', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.hook_mlp_in', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, example):\n",
    "    token_ids = model.to_tokens(example[\"text\"])\n",
    "    correct_token_id = model.to_single_token(example[\"correct\"])\n",
    "    incorrect_token_id = model.to_single_token(example[\"incorrect\"])\n",
    "    logit_diff_direction = (\n",
    "        model.W_U[:, correct_token_id] - model.W_U[:, incorrect_token_id]\n",
    "    )  # (d_model,)\n",
    "\n",
    "    _, cache = model.run_with_cache(\n",
    "        token_ids,\n",
    "        names_filter=lambda name: (\n",
    "            name in resid_names\n",
    "            or name == \"blocks.0.attn.hook_result\"\n",
    "            or name == \"blocks.2.attn.hook_result\"\n",
    "            or name == \"ln_final.hook_scale\"\n",
    "            or name == \"ln_final.hook_normalized\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Get activations from the cache\n",
    "    resids = torch.stack(\n",
    "        [cache[name] for name in resid_names], dim=0\n",
    "    )  # (resid, batch, pos, d_model)\n",
    "    L0H2 = cache[\"blocks.0.attn.hook_result\"][:, :, 2, :]  # (batch, pos, d_model)\n",
    "    L2HX = einops.reduce(\n",
    "        cache[\"blocks.2.attn.hook_result\"],\n",
    "        \"batch pos head d_model -> batch pos d_model\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    scale = cache[\"ln_final.hook_scale\"]\n",
    "\n",
    "    apply_ln = lambda x: (x - x.mean(dim=-1, keepdim=True)) / scale\n",
    "    resids_ln = apply_ln(resids)\n",
    "    L0H2_ln = apply_ln(L0H2)\n",
    "    L2HX_ln = apply_ln(L2HX)\n",
    "    \n",
    "    # Check that manual layernorming is correct\n",
    "    assert torch.allclose(cache[\"ln_final.hook_normalized\"], resids_ln[-1], atol=1e-5)\n",
    "\n",
    "    return (\n",
    "        resids_ln[:, 0, -1, :] @ logit_diff_direction,\n",
    "        projection_ratio(resids[:, 0, -1, :], L0H2[0, -1, :].unsqueeze(0)),\n",
    "        L0H2_ln[0, -1, :] @ logit_diff_direction,\n",
    "        L2HX_ln[0, -1, :] @ logit_diff_direction,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanup_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
